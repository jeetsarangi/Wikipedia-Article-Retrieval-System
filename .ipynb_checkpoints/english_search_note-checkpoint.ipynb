{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731e8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import linecache\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "#from English_indexer import *\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4a3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tags=''\n",
    "stemmer=''\n",
    "stop_words=''\n",
    "\n",
    "def remove_stopwords(text_data):\n",
    "\n",
    "    cleaned_text = [word for word in text_data if word not in stop_words]\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def stem_text(text_data):\n",
    "\n",
    "    cleaned_text = [stemmer.stem(x) for x in text_data]\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_non_ascii(text_data):\n",
    "    text_data  = text_data.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    cleaned_text = ''.join([i if ord(i) < 128 else ' ' for i in text_data])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_html_tags(text_data):\n",
    "\n",
    "    cleaned_text = re.sub(html_tags, ' ', text_data)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_special_chars(text_data):\n",
    "\n",
    "    cleaned_text = ''.join(ch if ch.isalnum() else ' ' for ch in text_data)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_select_keywords(text_data):\n",
    "\n",
    "    text_data = text_data.replace('\\n', ' ').replace('File:', ' ')\n",
    "    text_data = re.sub('(http://[^ ]+)', ' ', text_data)\n",
    "    text_data = re.sub('(https://[^ ]+)', ' ', text_data)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "def tokenize_sentence(text_data, flag=False):\n",
    "\n",
    "    if flag:\n",
    "        text_data = remove_select_keywords(text_data)\n",
    "        text_data = re.sub('\\{.*?\\}|\\[.*?\\]|\\=\\=.*?\\=\\=', ' ', text_data)\n",
    "    cleaned_text = remove_non_ascii(text_data)\n",
    "    cleaned_text = remove_html_tags(cleaned_text)\n",
    "    cleaned_text = remove_special_chars(cleaned_text)\n",
    "\n",
    "    return cleaned_text.split()\n",
    "\n",
    "def preprocess_text(text_data, flag=False):\n",
    "\n",
    "    cleaned_data = tokenize_sentence(text_data.lower(), flag)\n",
    "    cleaned_data = remove_stopwords(cleaned_data)\n",
    "    cleaned_data = stem_text(cleaned_data)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0638fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_token_info(high, filename, inp_token):\n",
    "    low = 0\n",
    "    while low < high:\n",
    "\n",
    "        mid = (low + high)//2\n",
    "        line = linecache.getline(filename, mid)\n",
    "        token = line.split('-')[0]\n",
    "\n",
    "        if inp_token == token:\n",
    "            token_info = line.split('-')[1:-1]\n",
    "            return token_info\n",
    "\n",
    "        elif inp_token > token:\n",
    "            low = mid + 1\n",
    "\n",
    "        else:\n",
    "            high = mid\n",
    "\n",
    "    return None\n",
    "def title_search(page_id):\n",
    "\n",
    "    title = linecache.getline('english_wiki_index/id_title_map.txt', int(page_id)+1).strip()\n",
    "    title = title.split('-', 1)[1]\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "def search_field_file(field, file_num, line_num):\n",
    "\n",
    "    if line_num != '':\n",
    "        line = linecache.getline(f'english_wiki_index/{field}_data_{str(file_num)}.txt', int(line_num)).strip()\n",
    "        postings = line.split('-')[1]\n",
    "\n",
    "        return postings\n",
    "\n",
    "    return ''\n",
    "\n",
    "def get_token_info(token):\n",
    "\n",
    "    char_list = [chr(i) for i in range(97,123)]\n",
    "    num_list = [str(i) for i in range(0,10)]\n",
    "\n",
    "    if token[0] in char_list:\n",
    "        with open(f'english_wiki_index/tokens_info_{token[0]}_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer =f'english_wiki_index/tokens_info_{token[0]}.txt'\n",
    "        token_info = binary_search_token_info(num_tokens, tokens_info_pointer, token)\n",
    "\n",
    "    elif token[0] in num_list:\n",
    "        with open(f'english_wiki_index/tokens_info_{token[0]}_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer = f'english_wiki_index/tokens_info_{token[0]}.txt'\n",
    "        token_info = binary_search_token_info(num_tokens, tokens_info_pointer, token)\n",
    "\n",
    "    else:\n",
    "        with open(f'english_wiki_index/tokens_info_others_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer = f'english_wiki_index/tokens_info_others.txt'\n",
    "        token_info = binary_search_token_info(num_tokens, tokens_info_pointer, token)\n",
    "\n",
    "    return token_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09feba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ranking(page_freq, page_postings):\n",
    "\n",
    "    result = defaultdict(float)\n",
    "    weightage_dict = {'title':1.0, 'body':0.6, 'category':0.4, 'infobox':0.75, 'link':0.20, 'reference':0.25}\n",
    "\n",
    "    for token, field_post_dict in page_postings.items():\n",
    "\n",
    "        for field, postings in field_post_dict.items():\n",
    "\n",
    "            weightage = weightage_dict[field]\n",
    "\n",
    "            if len(postings)>0:\n",
    "                for post in postings.split(';'):\n",
    "\n",
    "                    id, post = post.split(':')\n",
    "                    result[id] += weightage*(1+math.log(int(post)))*math.log((num_pages-int(page_freq[token]))/int(page_freq[token]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c541038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_query(preprocessed_query):\n",
    "\n",
    "    page_freq, page_postings = {}, defaultdict(dict)\n",
    "\n",
    "    for token in preprocessed_query:\n",
    "        token_info = get_token_info(token)\n",
    "\n",
    "        if token_info:\n",
    "            file_num, freq, title_line, body_line, category_line, infobox_line, link_line, reference_line = token_info\n",
    "            line_map = {\n",
    "                    'title' : title_line, 'body' : body_line, 'category' : category_line, 'infobox' : infobox_line, 'link' : link_line, 'reference' : reference_line\n",
    "                }\n",
    "\n",
    "            for field_name, line_num in line_map.items():\n",
    "\n",
    "                if line_num!='':\n",
    "                    posting = search_field_file(field_name, file_num, line_num)\n",
    "\n",
    "                    page_freq[token] = len(posting.split(';'))\n",
    "                    page_postings[token][field_name] = posting\n",
    "\n",
    "\n",
    "    return page_freq, page_postings\n",
    "\n",
    "\n",
    "def field_query(preprocessed_query):\n",
    "\n",
    "    page_freq, page_postings = {}, defaultdict(dict)\n",
    "\n",
    "    for field, token in preprocessed_query:\n",
    "        token_info = get_token_info(token)\n",
    "\n",
    "        if token_info:\n",
    "            file_num, freq, title_line, body_line, category_line, infobox_line, link_line, reference_line = token_info\n",
    "            line_map = {\n",
    "                'title':title_line, 'body':body_line, 'category':category_line, 'infobox':infobox_line, 'link':link_line, 'reference':reference_line\n",
    "            }\n",
    "            field_map = {\n",
    "                't':'title', 'b':'body', 'c':'category', 'i':'infobox', 'l':'link', 'r':'reference'\n",
    "            }\n",
    "\n",
    "            field_name = field_map[field]\n",
    "            line_num = line_map[field_name]\n",
    "\n",
    "            posting = search_field_file(field_name, file_num, line_num)\n",
    "            page_freq[token] = len(posting)\n",
    "            page_postings[token][field_name] = posting\n",
    "\n",
    "    return page_freq, page_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d07ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_query_type(query):\n",
    "    field_replace_map = {\n",
    "            ' t:':';t:',\n",
    "            ' b:':';b:',\n",
    "            ' c:':';c:',\n",
    "            ' i:':';i:',\n",
    "            ' l:':';l:',\n",
    "            ' r:':';r:',\n",
    "        }\n",
    "\n",
    "    if ('t:' in query or 'b:' in query or 'c:' in query or 'i:' in query or 'l:' in query or 'r:' in query) and query[0:2] not in ['t:', 'b:', 'i:', 'c:', 'r:', 'l:']:\n",
    "\n",
    "        for k, v in field_replace_map.items():\n",
    "            if k in query:\n",
    "                query = query.replace(k, v)\n",
    "\n",
    "        query = query.lstrip(';')\n",
    "\n",
    "        return query.split(';')[0], query.split(';')[1:]\n",
    "\n",
    "    elif 't:' in query or 'b:' in query or 'c:' in query or 'i:' in query or 'l:' in query or 'r:' in query:\n",
    "\n",
    "        for k, v in field_replace_map.items():\n",
    "            if k in query:\n",
    "                query = query.replace(k, v)\n",
    "\n",
    "        query = query.lstrip(';')\n",
    "\n",
    "        return query.split(';'), None\n",
    "\n",
    "    else:\n",
    "        return query, None\n",
    "def return_query_results(query, query_type):\n",
    "    if query_type=='field':\n",
    "        preprocessed_query = [[qry.split(':')[0], preprocess_text(qry.split(':')[1])] for qry in query]\n",
    "    else:\n",
    "        preprocessed_query = preprocess_text(query)\n",
    "\n",
    "    if query_type == 'field':\n",
    "\n",
    "        preprocessed_query_final = []\n",
    "        for field, words in preprocessed_query:\n",
    "            for word in words:\n",
    "                preprocessed_query_final.append([field, word])\n",
    "\n",
    "        page_freq, page_postings = field_query(preprocessed_query_final)\n",
    "\n",
    "    else:\n",
    "\n",
    "        page_freq, page_postings = simple_query(preprocessed_query)\n",
    "\n",
    "    ranked_results = do_ranking(page_freq, page_postings)\n",
    "\n",
    "    return ranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e120b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input_from_file(file_name, num_results):\n",
    "    results_file = file_name.split('.txt')[0]\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        fp = open(results_file+'_op.txt', 'w')\n",
    "        for i, query in enumerate(f):\n",
    "            s = time.time()\n",
    "\n",
    "            query = query.strip()\n",
    "            query1, query2 = identify_query_type(query)\n",
    "\n",
    "            if query2:\n",
    "                ranked_results1 = return_query_results(query1, 'simple')\n",
    "\n",
    "                ranked_results2 = return_query_results(query2, 'field')\n",
    "\n",
    "                ranked_results = Counter(ranked_results1) + Counter(ranked_results2)\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            elif type(query1)==type([]):\n",
    "\n",
    "                ranked_results = return_query_results(query1, 'field')\n",
    "\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            else:\n",
    "                ranked_results = return_query_results(query1, 'simple')\n",
    "\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            e = time.time()\n",
    "            fp.write('Finished in ' + str(e-s) + ' seconds')\n",
    "            fp.write('\\n\\n')\n",
    "\n",
    "            print('Done query', i+1)\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "    print('Done writing results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58dcfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input_from_user(num_results):\n",
    "    start = time.time()\n",
    "\n",
    "    while True:\n",
    "        query = input('Enter Query:- ')\n",
    "        if query=='close':\n",
    "            return\n",
    "        s = time.time()\n",
    "\n",
    "        query = query.strip()\n",
    "        query1, query2 = identify_query_type(query)\n",
    "\n",
    "        if query2:\n",
    "            ranked_results1 = return_query_results(query1, 'simple')\n",
    "\n",
    "            ranked_results2 = return_query_results(query2, 'field')\n",
    "\n",
    "            ranked_results = Counter(ranked_results1) + Counter(ranked_results2)\n",
    "            results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "            results = results[:num_results]\n",
    "\n",
    "            for id, _ in results:\n",
    "                title= title_search(id)\n",
    "                print(id+',', title)\n",
    "\n",
    "        elif type(query1)==type([]):\n",
    "\n",
    "            ranked_results = return_query_results(query1, 'field')\n",
    "\n",
    "            results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "            results = results[:num_results]\n",
    "\n",
    "            for id, _ in results:\n",
    "                title= title_search(id)\n",
    "                print(id+',', title)\n",
    "\n",
    "        else:\n",
    "            ranked_results = return_query_results(query1, 'simple')\n",
    "\n",
    "            results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "            results = results[:num_results]\n",
    "\n",
    "            for id, _ in results:\n",
    "                title= title_search(id)\n",
    "                link = ('https://en.wikipedia.org/wiki/'+ title.title())\n",
    "                link = link.replace(' ','_')\n",
    "                print(id+',', title, ', ', link)\n",
    "\n",
    "        e = time.time()\n",
    "        print('Finished in', e-s, 'seconds')\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f9738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "file_name = None\n",
    "num_results = 10\n",
    "print('Loading search engine ')\n",
    "\n",
    "stop_words = (set(stopwords.words(\"english\")))\n",
    "html_tags = re.compile('&amp;|&apos;|&gt;|&lt;|&nbsp;|&quot;')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "with open('english_wiki_index/num_pages.txt', 'r') as f:\n",
    "    num_pages = float(f.readline().strip())\n",
    "\n",
    "#temp = linecache.getline('english_wiki_index/id_title_map.txt', 0)\n",
    "\n",
    "print('Loaded in', time.time() - start, 'seconds')\n",
    "\n",
    "print('Starting Querying')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if file_name is not None:\n",
    "    take_input_from_file(file_name, num_results)\n",
    "else:\n",
    "    take_input_from_user(num_results)\n",
    "\n",
    "\n",
    "print('Done querying in', time.time() - start, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "indentWithTabs": true
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
