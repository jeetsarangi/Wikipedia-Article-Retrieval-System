{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc4e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import heapq\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import xml.sax\n",
    "import argparse\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4007eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tags = re.compile('&amp;|&apos;|&gt;|&lt;|&nbsp;|&quot;')\n",
    "stop_words = (set(stopwords.words(\"english\")))\n",
    "stemmer = PorterStemmer()\n",
    "index_map = defaultdict(str)\n",
    "page_no = 0\n",
    "file_no = 0\n",
    "id_to_title = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e25525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(text_data):\n",
    "    \n",
    "    text = [word for word in text_data if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "def stem_word(word):\n",
    "\n",
    "    for wrd in stem_words:\n",
    "        if word.endswith(wrd):\n",
    "            word = word[:-len(wrd)]\n",
    "            return word\n",
    "    return word\n",
    "\n",
    "def stem(text_data):\n",
    "\n",
    "    cleaned_text = [stem_word(word) for word in text_data]\n",
    "    return cleaned_text\n",
    "def remove_non_ascii(text):\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "    return text\n",
    "def remove_html_tags(text):\n",
    "    text = re.sub(html_tags, ' ', text)\n",
    "    return text\n",
    "def remove_special_chars(text):\n",
    "    \n",
    "    text = ''.join(ch if ch.isalnum() else ' ' for ch in text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a654c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, flag=False):\n",
    "\n",
    "    if flag:\n",
    "#         text = self.remove_select_keywords(text)\n",
    "        text = re.sub('\\{.*?\\}|\\[.*?\\]|\\=\\=.*?\\=\\=', ' ', text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_chars(text)\n",
    "\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4617e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, flag=False):\n",
    "\n",
    "    text = tokenize(text.lower(), flag)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem(text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a60244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTitle(text):\n",
    "\n",
    "        data = tokenize(text)\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    "\n",
    "def extractInfobox(text):\n",
    "\n",
    "        data = text.split('\\n')\n",
    "        flag = 0\n",
    "        info = []\n",
    "        for line in data:\n",
    "            if re.match(r'\\{\\{infobox', line):\n",
    "                flag = 1\n",
    "                info.append(re.sub(r'\\{\\{infobox(.*)', r'\\1', line))\n",
    "            elif flag == 1:\n",
    "                if line == '}}':\n",
    "                    flag = 0\n",
    "                    continue\n",
    "                info.append(line)\n",
    "\n",
    "        data = tokenize(' '.join(info))\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    "\n",
    "def extractReferences(text):\n",
    "\n",
    "        data = text.split('\\n')\n",
    "        refs = []\n",
    "        for line in data:\n",
    "            if re.search(r'<ref', line):\n",
    "                refs.append(re.sub(r'.*title[\\ ]*=[\\ ]*([^\\|]*).*', r'\\1', line))\n",
    "\n",
    "        data = tokenize(' '.join(refs))\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    "def extractCategories(text):\n",
    "        \n",
    "        data = text.split('\\n')\n",
    "        categories = []\n",
    "        for line in data:\n",
    "            if re.match(r'\\[\\[category', line):\n",
    "                categories.append(re.sub(r'\\[\\[category:(.*)\\]\\]', r'\\1', line))\n",
    "        \n",
    "        data = tokenize(' '.join(categories))\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    "def extractExternalLinks(text):\n",
    "        \n",
    "        data = text.split('\\n')\n",
    "        links = []\n",
    "        for line in data:\n",
    "            if re.match(r'\\*[\\ ]*\\[', line):\n",
    "                links.append(line)\n",
    "        \n",
    "        data = tokenize(' '.join(links))\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    " \n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data\n",
    "\n",
    "def extractBody( text):\n",
    "\n",
    "        data = re.sub(r'\\{\\{.*\\}\\}', r' ', text)\n",
    "        \n",
    "        data = tokenize(data)\n",
    "        data = removeStopWords(data)\n",
    "        data = stem(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e3769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text, title):\n",
    "        \n",
    "        text = text.lower() #Case Folding\n",
    "        data = text.split('==references==')\n",
    "        if len(data) == 1:\n",
    "            data = text.split('== references == ')\n",
    "        if len(data) == 1:\n",
    "            references = []\n",
    "            links = []\n",
    "            categories = []\n",
    "        else:\n",
    "            references = extractReferences(data[1])\n",
    "            links = extractExternalLinks(data[1])\n",
    "            categories = extractCategories(data[1])\n",
    "        info = extractInfobox(data[0])\n",
    "        body = extractBody(data[0])\n",
    "        title = extractTitle(title.lower())\n",
    "    \n",
    "        return title, body, info, categories, links, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5be163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(xml.sax.ContentHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tag = ''\n",
    "        self.title = ''\n",
    "        self.text = ''\n",
    "        \n",
    "    def startElement(self,tag,a):\n",
    "        self.tag=tag\n",
    "        \n",
    "    def characters(self, content):\n",
    "        \n",
    "        if self.tag == 'title':\n",
    "            self.title += content\n",
    "\n",
    "        if self.tag == 'text':\n",
    "            self.text += content\n",
    "    \n",
    "    def endElement(self,name):\n",
    "        \n",
    "        if name=='page':\n",
    "            \n",
    "            print(page_no)\n",
    "            tit = self.title.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "            title, body, category, infobox, link, reference = processText(tit, self.text)\n",
    "            id_to_title[page_no]=tit.lower()\n",
    "            create_index(title, body, category, infobox, link, reference)\n",
    "\n",
    "            self.tag = \"\"\n",
    "            self.title = \"\"\n",
    "            self.text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301679a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_postings(token, postings, final_tag):\n",
    "\n",
    "        postings = sorted(postings.items(), key = lambda item : int(item[0]))\n",
    "\n",
    "        final_posting = token+'-'\n",
    "        for id, freq in postings:\n",
    "            final_posting+=str(id)+':'+freq+';'\n",
    "\n",
    "        final_tag.append(final_posting.rstrip(';'))\n",
    "\n",
    "        return final_tag\n",
    "\n",
    "def write_diff_postings(tag_type, final_tag, num_files_final):\n",
    "\n",
    "    with open(f'hindi_wiki_index/{tag_type}_data_{str(num_files_final)}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(final_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06532d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_final_files(data_to_merge, num_files_final):\n",
    "\n",
    "        title_dict, body_dict, category_dict, infobox_dict, link_dict, reference_dict = defaultdict(dict), defaultdict(dict), defaultdict(dict), defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "        unique_tokens_info = {}\n",
    "\n",
    "        sorted_data = sorted(data_to_merge.items(), key = lambda item : item[0])\n",
    "\n",
    "        for i, (token, postings) in enumerate(sorted_data):\n",
    "            for posting in postings.split(';')[:-1]:\n",
    "\n",
    "                id = posting.split(':')[0]\n",
    "\n",
    "                fields = posting.split(':')[1]\n",
    "\n",
    "                if 't' in fields:\n",
    "                    title_dict[token][id] = re.search(r'.*t([0-9]*).*', fields).group(1)\n",
    "\n",
    "                if 'b' in fields:\n",
    "                    body_dict[token][id] = re.search(r'.*b([0-9]*).*', fields).group(1)\n",
    "\n",
    "                if 'c' in fields:\n",
    "                    category_dict[token][id] = re.search(r'.*c([0-9]*).*', fields).group(1)\n",
    "\n",
    "                if 'i' in fields:\n",
    "                    infobox_dict[token][id] = re.search(r'.*i([0-9]*).*', fields).group(1)\n",
    "\n",
    "                if 'l' in fields:\n",
    "                    link_dict[token][id] = re.search(r'.*l([0-9]*).*', fields).group(1)\n",
    "\n",
    "                if 'r' in fields:\n",
    "                    reference_dict[token][id] = re.search(r'.*r([0-9]*).*', fields).group(1)\n",
    "\n",
    "            token_info = '-'.join([token, str(num_files_final), str(len(postings.split(';')[:-1]))])\n",
    "            unique_tokens_info[token] = token_info+'-'\n",
    "\n",
    "        final_titles, final_body_texts, final_categories, final_infoboxes, final_links, final_references = [], [], [], [], [], []\n",
    "\n",
    "        for i, (token, _) in enumerate(sorted_data):\n",
    "\n",
    "            if token in title_dict.keys():\n",
    "                posting = title_dict[token]\n",
    "                final_titles = get_diff_postings(token, posting, final_titles)\n",
    "                t = len(final_titles)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "            if token in body_dict.keys():\n",
    "                posting = body_dict[token]\n",
    "                final_body_texts = get_diff_postings(token, posting, final_body_texts)\n",
    "                t = len(final_body_texts)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "            if token in category_dict.keys():\n",
    "                posting = category_dict[token]\n",
    "                final_categories = get_diff_postings(token, posting, final_categories)\n",
    "                t = len(final_categories)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "            if token in infobox_dict.keys():\n",
    "                posting = infobox_dict[token]\n",
    "                final_infoboxes = get_diff_postings(token, posting, final_infoboxes)\n",
    "                t = len(final_infoboxes)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "            if token in link_dict.keys():\n",
    "                posting = link_dict[token]\n",
    "                final_links = get_diff_postings(token, posting, final_links)\n",
    "                t = len(final_links)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "            if token in reference_dict.keys():\n",
    "                posting = reference_dict[token]\n",
    "                final_references = get_diff_postings(token, posting, final_references)\n",
    "                t = len(final_references)\n",
    "                unique_tokens_info[token]+=str(t)+'-'\n",
    "            else:\n",
    "                unique_tokens_info[token]+='-'\n",
    "\n",
    "        with open(f'hindi_wiki_index/tokens_info.txt', 'a') as f:\n",
    "            f.write('\\n'.join(unique_tokens_info.values()))\n",
    "            f.write('\\n')\n",
    "\n",
    "        write_diff_postings('title', final_titles, num_files_final)\n",
    "\n",
    "        write_diff_postings('body', final_body_texts, num_files_final)\n",
    "\n",
    "        write_diff_postings('category', final_categories, num_files_final)\n",
    "\n",
    "        write_diff_postings('infobox', final_infoboxes, num_files_final)\n",
    "\n",
    "        write_diff_postings('link', final_links, num_files_final)\n",
    "\n",
    "        write_diff_postings('reference', final_references, num_files_final)\n",
    "\n",
    "        num_files_final+=1\n",
    "\n",
    "        return num_files_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3de472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_files(filecount):\n",
    "    files = {}\n",
    "    lines = {}\n",
    "    postings = {}\n",
    "    open_files = [0]*filecount\n",
    "    top_words = []\n",
    "    \n",
    "    for i in range(filecount):\n",
    "        files[i] = open(f'hindi_wiki_index/index_{i}.txt', 'r')\n",
    "        lines[i] = files[i].readline().strip('\\n')\n",
    "        postings[i] = lines[i].split('-')\n",
    "        open_files[i] = 1\n",
    "        token = postings[i][0]\n",
    "        \n",
    "        if token not in top_words:\n",
    "            heapq.heappush(top_words,token)\n",
    "    \n",
    "    num_files = 0\n",
    "    num_processed_postings = 0\n",
    "    merge_data = defaultdict(str)\n",
    "    \n",
    "    while any(open_files) == 1:\n",
    "        token = heapq.heappop(top_words)\n",
    "        num_processed_postings+=1\n",
    "        \n",
    "        if num_processed_postings%30000==0:\n",
    "            num_files = write_final_files(merge_data, num_files)\n",
    "            merge_data = defaultdict(str)\n",
    "            \n",
    "        i=0\n",
    "        while i < filecount:\n",
    "            \n",
    "            if open_files[i] == 1:\n",
    "                \n",
    "                if postings[i][0] == token:\n",
    "                    merge_data[token]+=(postings[i][1])\n",
    "                    lines[i] = files[i].readline().strip('\\n')\n",
    "                    \n",
    "                    if len(lines[i]):\n",
    "                        postings[i]=lines[i].split('-')\n",
    "                        new_token = postings[i][0]\n",
    "                        \n",
    "                        if new_token not in top_words:\n",
    "                            heapq.heappush(top_words, new_token)\n",
    "                    else:\n",
    "                        open_files[i] = 0\n",
    "                        files[i].close()\n",
    "                        print(f'Removing file {str(i)}')\n",
    "                        os.remove(f'hindi_wiki_index/index_{str(i)}.txt')\n",
    "                        \n",
    "            i += 1\n",
    "    num_files = write_final_files(merge_data, num_files)\n",
    "    return num_files    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e7a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_inter():\n",
    "    global file_no\n",
    "    temp_index = sorted(index_map.items() , key = lambda item : item[0])\n",
    "    \n",
    "    res = []\n",
    "    for word, post in temp_index:\n",
    "        t = word+\"-\"+post\n",
    "        res.append(t)\n",
    "    with open(f'hindi_wiki_index/index_{file_no}.txt','w') as f:\n",
    "        f.write('\\n'.join(res))\n",
    "    file_no += 1\n",
    "#     if(file_no == 10):\n",
    "#         merge_files(file_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bdbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_id_title():\n",
    "    temp_map = sorted(id_to_title.items(),key = lambda item: int(item[0]))\n",
    "    res = []\n",
    "    for id, title in temp_map:\n",
    "        t = str(id)+'-'+title.strip()\n",
    "        res.append(t)\n",
    "        \n",
    "    with open(f\"hindi_wiki_index/id_title_map.txt\", 'a') as f:\n",
    "            f.write('\\n'.join(res))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3c67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(title, body, category, infobox, link, reference):\n",
    "    global page_no\n",
    "    global index_map\n",
    "    \n",
    "    unique_words = set()\n",
    "    \n",
    "    title_dict = defaultdict(int)\n",
    "    for w in title:\n",
    "        title_dict[w] += 1\n",
    "    \n",
    "    body_dict = defaultdict(int)\n",
    "    for w in body:\n",
    "        body_dict[w] += 1\n",
    "    \n",
    "    category_dict = defaultdict(int)\n",
    "    for w in category:\n",
    "        category_dict[w] += 1\n",
    "        \n",
    "    infobox_dict = defaultdict(int)\n",
    "    for w in infobox:\n",
    "        infobox_dict[w] += 1\n",
    "        \n",
    "    link_dict = defaultdict(int)\n",
    "    for w in link:\n",
    "        link_dict[w] += 1\n",
    "        \n",
    "    reference_dict = defaultdict(int)\n",
    "    for w in reference:\n",
    "        reference_dict[w] += 1\n",
    "    \n",
    "    unique_words.update(title)\n",
    "    unique_words.update(body)\n",
    "    unique_words.update(category)\n",
    "    unique_words.update(infobox)\n",
    "    unique_words.update(reference)\n",
    "    \n",
    "    for word in unique_words:\n",
    "        temp = re.sub(r'^((.)(?!\\2\\2\\2))+$',r'\\1', word)\n",
    "        if len(temp) != len(word):\n",
    "            posting = str(page_no)+\":\"\n",
    "            if title_dict[word]:\n",
    "                posting += 't'+str(title_dict[word])\n",
    "\n",
    "            if body_dict[word]:\n",
    "                posting += 'b'+str(body_dict[word])\n",
    "\n",
    "            if category_dict[word]:\n",
    "                posting += 'c'+str(category_dict[word])\n",
    "\n",
    "            if infobox_dict[word]:\n",
    "                posting += 'i'+str(infobox_dict[word])\n",
    "\n",
    "            if link_dict[word]:\n",
    "                posting += 'l'+str(link_dict[word])\n",
    "\n",
    "            if reference_dict[word]:\n",
    "                posting += 'r'+str(reference_dict[word])\n",
    "            posting += \";\"\n",
    "            \n",
    "            index_map[word] += posting\n",
    "            \n",
    "    page_no += 1\n",
    "    if page_no%20000 == 0:\n",
    "        write_id_title()\n",
    "        write_inter()\n",
    "        \n",
    "        index_map = defaultdict(str)\n",
    "        id_to_title = {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hindi_stem_words.txt', 'r', encoding='utf8') as f:\n",
    "    stem_words = [word.strip() for word in f]\n",
    "\n",
    "with open('hindi_stopwords.txt', 'r',encoding='utf8') as f:\n",
    "    stop_words = [word.strip() for word in f]\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces,False)\n",
    "content_handler = Handler()\n",
    "parser.setContentHandler(content_handler)\n",
    "\n",
    "\n",
    "parser.parse(\"hindi-data.xml\")\n",
    "\n",
    "\n",
    "write_inter()\n",
    "write_id_title()\n",
    "merge_files(file_no)\n",
    "\n",
    "with open('hindi_wiki_index/num_pages.txt', 'w',encoding='utf8') as f:\n",
    "    f.write(str(page_no))\n",
    "\n",
    "num_tokens_final = 0\n",
    "with open('hindi_wiki_index/tokens_info.txt', 'r',encoding='utf8') as f:\n",
    "    for line in tqdm(f):\n",
    "        num_tokens_final+=1\n",
    "\n",
    "with open('hindi_wiki_index/num_tokens.txt', 'w',encoding='utf8') as f:\n",
    "    f.write(str(num_tokens_final))\n",
    "\n",
    "print(num_tokens_final, page_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d0157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
