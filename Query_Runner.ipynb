{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c323e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "import linecache\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "#from english_indexer import *\n",
    "import re\n",
    "import heapq\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import xml.sax\n",
    "import time\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "from hindi_search_note_mod import run_hindi_search\n",
    "\n",
    "\n",
    "html_tags = re.compile('&amp;|&apos;|&gt;|&lt;|&nbsp;|&quot;')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words=''\n",
    "stem_words = ''\n",
    "num_pages = 1\n",
    "num_tokens = 1\n",
    "lang = 0\n",
    "\n",
    "def remove_stopwords(text_data):\n",
    "\n",
    "    cleaned_text = [word for word in text_data if word not in stop_words]\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def stem_word(word):\n",
    "    for wrd in stem_words:\n",
    "        if word.endswith(wrd):\n",
    "            word = word[:-len(wrd)]\n",
    "            return word\n",
    "    return word\n",
    "\n",
    "def stem_text(text_data):\n",
    "    cleaned_text = ''\n",
    "    if lang:\n",
    "        cleaned_text = [stem_word(word) for word in text_data]\n",
    "    else:\n",
    "        cleaned_text = [stemmer.stem(x) for x in text_data]\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_non_ascii(text_data):\n",
    "#     text_data  = text_data.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    cleaned_text = ''.join([i if ord(i) < 128 else ' ' for i in text_data])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_html_tags(text_data):\n",
    "\n",
    "    cleaned_text = re.sub(html_tags, ' ', text_data)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_special_chars(text_data):\n",
    "\n",
    "    cleaned_text = ''.join(ch if ch.isalnum() else ' ' for ch in text_data)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_select_keywords(text_data):\n",
    "\n",
    "    text_data = text_data.replace('\\n', ' ').replace('File:', ' ')\n",
    "    text_data = re.sub('(http://[^ ]+)', ' ', text_data)\n",
    "    text_data = re.sub('(https://[^ ]+)', ' ', text_data)\n",
    "\n",
    "    return text_data\n",
    "\n",
    "def tokenize_sentence(text_data, flag=False):\n",
    "\n",
    "    if flag:\n",
    "        text_data = remove_select_keywords(text_data)\n",
    "        text_data = re.sub('\\{.*?\\}|\\[.*?\\]|\\=\\=.*?\\=\\=', ' ', text_data)\n",
    "    cleaned_text = remove_non_ascii(text_data)\n",
    "    cleaned_text = remove_html_tags(cleaned_text)\n",
    "    cleaned_text = remove_special_chars(cleaned_text)\n",
    "\n",
    "    return cleaned_text.split()\n",
    "\n",
    "def preprocess_text(text_data, flag=False):\n",
    "\n",
    "    cleaned_data = tokenize_sentence(text_data.lower(), flag)\n",
    "    cleaned_data = remove_stopwords(cleaned_data)\n",
    "    cleaned_data = stem_text(cleaned_data)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71216319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_binary(h, filename, input1):\n",
    "    l = 0\n",
    "    while l < h:\n",
    "        mid = (l + h)//2\n",
    "        line = linecache.getline(filename, mid)\n",
    "        token = line.split('-')[0]\n",
    "        if input1 == token:\n",
    "            return line.split('-')[1:-1]\n",
    "        elif input1 > token:\n",
    "            l = mid + 1\n",
    "        else:\n",
    "            h = mid\n",
    "\n",
    "    return None\n",
    "def title_search(page_id):\n",
    "    if lang:\n",
    "        t = linecache.getline('hindi_wiki_index/id_title_map.txt', int(page_id)+1).strip()\n",
    "        t = t.split('-', 1)[1]\n",
    "        return t\n",
    "    else:\n",
    "        t = linecache.getline('english_wiki_index/id_title_map.txt', int(page_id)+1).strip()\n",
    "        t = t.split('-', 1)[1]\n",
    "        return t\n",
    "\n",
    "\n",
    "def search_field_file(field, file_num, line_num):\n",
    "    if lang:\n",
    "        if line_num != '':\n",
    "            line = linecache.getline(f'hindi_wiki_index/{field}_data_{str(file_num)}.txt', int(line_num)).strip()\n",
    "            postings = line.split('-')[1]\n",
    "            return postings\n",
    "        return ''\n",
    "    else:\n",
    "        if line_num != '':\n",
    "            line = linecache.getline(f'english_wiki_index/{field}_data_{str(file_num)}.txt', int(line_num)).strip()\n",
    "            postings = line.split('-')[1]\n",
    "            return postings\n",
    "        return ''\n",
    "\n",
    "def get_token_info(token):\n",
    "    if lang:\n",
    "        with open('hindi_wiki_index/num_tokens.txt', 'r', encoding='utf8') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "        token_info_pointer =  'hindi_wiki_index/tokens_info.txt'\n",
    "        token_info =  search_binary(num_tokens, token_info_pointer, token)\n",
    "        return token_info\n",
    "    \n",
    "    chars = [chr(i) for i in range(97,123)] \n",
    "    nums = [str(i) for i in range(0,10)]\n",
    "    \n",
    "\n",
    "\n",
    "    if token[0] in nums:\n",
    "        with open(f'english_wiki_index/tokens_info_{token[0]}_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer = f'english_wiki_index/tokens_info_{token[0]}.txt'\n",
    "        token_info = search_binary(num_tokens, tokens_info_pointer, token)\n",
    "        \n",
    "    \n",
    "    elif token[0] in chars:\n",
    "        with open(f'english_wiki_index/tokens_info_{token[0]}_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer =f'english_wiki_index/tokens_info_{token[0]}.txt'\n",
    "        token_info = search_binary(num_tokens, tokens_info_pointer, token)\n",
    "\n",
    "    else:\n",
    "        with open(f'english_wiki_index/tokens_info_others_count.txt', 'r') as f:\n",
    "            num_tokens = int(f.readline().strip())\n",
    "\n",
    "        tokens_info_pointer = f'english_wiki_index/tokens_info_others.txt'\n",
    "        token_info = search_binary(num_tokens, tokens_info_pointer, token)\n",
    "\n",
    "    return token_info\n",
    "\n",
    "\n",
    "\n",
    "def do_ranking(page_freq, page_postings):\n",
    "\n",
    "    result = defaultdict(float)\n",
    "    weightage_dict = {'title':1.0, 'body':0.6, 'category':0.4, 'infobox':0.75, 'link':0.20, 'reference':0.25}\n",
    "    for token, field_post_dict in page_postings.items():\n",
    "        for field, postings in field_post_dict.items():\n",
    "            weightage = weightage_dict[field]\n",
    "            if len(postings)>0:\n",
    "                for post in postings.split(';'):\n",
    "                    id, post = post.split(':')\n",
    "                    result[id] += weightage*(1+math.log(int(post)))*math.log((num_pages-int(page_freq[token]))/int(page_freq[token]))\n",
    "    return result\n",
    "\n",
    "def simple_query(preprocessed_query):\n",
    "\n",
    "    page_freq, page_postings = {}, defaultdict(dict)\n",
    "\n",
    "    for token in preprocessed_query:\n",
    "        token_info = get_token_info(token)\n",
    "\n",
    "        if token_info:\n",
    "            file_num, freq, title_line, body_line, category_line, infobox_line, link_line, reference_line = token_info\n",
    "            line_map = {\n",
    "                    'title' : title_line, 'body' : body_line, 'category' : category_line, 'infobox' : infobox_line, 'link' : link_line, 'reference' : reference_line\n",
    "                }\n",
    "\n",
    "            for field_name, line_num in line_map.items():\n",
    "                if line_num!='':\n",
    "                    posting = search_field_file(field_name, file_num, line_num)\n",
    "                    page_freq[token] = len(posting.split(';'))\n",
    "                    page_postings[token][field_name] = posting\n",
    "\n",
    "\n",
    "    return page_freq , page_postings\n",
    "\n",
    "\n",
    "def field_query(preprocessed_query):\n",
    "\n",
    "    page_freq, page_postings = {}, defaultdict(dict)\n",
    "\n",
    "    for field, token in preprocessed_query:\n",
    "        token_info = get_token_info(token)\n",
    "\n",
    "        if token_info:\n",
    "            file_num, freq, title_line, body_line, category_line, infobox_line, link_line, reference_line = token_info\n",
    "            line_map = {\n",
    "                'title':title_line, 'body':body_line, 'category':category_line, 'infobox':infobox_line, 'link':link_line, 'reference':reference_line\n",
    "            }\n",
    "            field_map = {\n",
    "                't':'title', 'b':'body', 'c':'category', 'i':'infobox', 'l':'link', 'r':'reference'\n",
    "            }\n",
    "\n",
    "            field_name = field_map[field]\n",
    "            line_num = line_map[field_name]\n",
    "\n",
    "            posting = search_field_file(field_name, file_num, line_num)\n",
    "            page_freq[token] = len(posting)\n",
    "            page_postings[token][field_name] = posting\n",
    "\n",
    "    return page_freq, page_postings\n",
    "\n",
    "def identify_query_type(query):\n",
    "    field_replace_map = {\n",
    "            ' t:':';t:',\n",
    "            ' b:':';b:',\n",
    "            ' c:':';c:',\n",
    "            ' i:':';i:',\n",
    "            ' l:':';l:',\n",
    "            ' r:':';r:',\n",
    "        }\n",
    "\n",
    "    if ('t:' in query or 'b:' in query or 'c:' in query or 'i:' in query or 'l:' in query or 'r:' in query) and query[0:2] not in ['t:', 'b:', 'i:', 'c:', 'r:', 'l:']:\n",
    "\n",
    "        for k, v in field_replace_map.items():\n",
    "            if k in query:\n",
    "                query = query.replace(k, v)\n",
    "\n",
    "        query = query.lstrip(';')\n",
    "\n",
    "        return query.split(';')[0], query.split(';')[1:]\n",
    "\n",
    "    elif 't:' in query or 'b:' in query or 'c:' in query or 'i:' in query or 'l:' in query or 'r:' in query:\n",
    "\n",
    "        for k, v in field_replace_map.items():\n",
    "            if k in query:\n",
    "                query = query.replace(k, v)\n",
    "\n",
    "        query = query.lstrip(';')\n",
    "\n",
    "        return query.split(';'), None\n",
    "\n",
    "    else:\n",
    "        return query, None\n",
    "def return_query_results(query, query_type):\n",
    "    if query_type=='field':\n",
    "        preprocessed_query = [[qry.split(':')[0], preprocess_text(qry.split(':')[1])] for qry in query]\n",
    "    else:\n",
    "        preprocessed_query = preprocess_text(query)\n",
    "\n",
    "    if query_type == 'field':\n",
    "\n",
    "        preprocessed_query_final = []\n",
    "        for field, words in preprocessed_query:\n",
    "            for word in words:\n",
    "                preprocessed_query_final.append([field, word])\n",
    "\n",
    "        page_freq, page_postings = field_query(preprocessed_query_final)\n",
    "\n",
    "    else:\n",
    "\n",
    "        page_freq, page_postings = simple_query(preprocessed_query)\n",
    "\n",
    "    ranked_results = do_ranking(page_freq, page_postings)\n",
    "\n",
    "    return ranked_results\n",
    "\n",
    "def take_input_from_file(file_name, num_results):\n",
    "    results_file = file_name.split('.txt')[0]\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        fp = open(results_file+'_op.txt', 'w')\n",
    "        for i, query in enumerate(f):\n",
    "            s = time.time()\n",
    "\n",
    "            query = query.strip()\n",
    "            query1, query2 = identify_query_type(query)\n",
    "\n",
    "            if query2:\n",
    "                ranked_results1 = return_query_results(query1, 'simple')\n",
    "\n",
    "                ranked_results2 = return_query_results(query2, 'field')\n",
    "\n",
    "                ranked_results = Counter(ranked_results1) + Counter(ranked_results2)\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            elif type(query1)==type([]):\n",
    "\n",
    "                ranked_results = return_query_results(query1, 'field')\n",
    "\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            else:\n",
    "                ranked_results = return_query_results(query1, 'simple')\n",
    "\n",
    "                results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "                results = results[:num_results]\n",
    "\n",
    "                if results:\n",
    "                    for id, _ in results:\n",
    "                        title= title_search(id)\n",
    "                        fp.write(id + ', ' + title)\n",
    "                        fp.write('\\n')\n",
    "                else:\n",
    "                    fp.write('No matching Doc found')\n",
    "                    fp.write('\\n')\n",
    "\n",
    "            e = time.time()\n",
    "            fp.write('Finished in ' + str(e-s) + ' seconds')\n",
    "            fp.write('\\n\\n')\n",
    "\n",
    "            print('Done query', i+1)\n",
    "\n",
    "        fp.close()\n",
    "\n",
    "    print('Done writing results')\n",
    "\n",
    "def take_input_from_user(query, num_results):\n",
    "    start = time.time()\n",
    "    if query=='close':\n",
    "        return\n",
    "    s = time.time()\n",
    "\n",
    "    query = query.strip()\n",
    "    query1, query2 = identify_query_type(query)\n",
    "\n",
    "    if query2:\n",
    "        ranked_results1 = return_query_results(query1, 'simple')\n",
    "\n",
    "        ranked_results2 = return_query_results(query2, 'field')\n",
    "\n",
    "        ranked_results = Counter(ranked_results1) + Counter(ranked_results2)\n",
    "        results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "        results = results[:num_results]\n",
    "\n",
    "        for id, _ in results:\n",
    "            title= title_search(id)\n",
    "            print(id+',', title)\n",
    "\n",
    "    elif type(query1)==type([]):\n",
    "\n",
    "        ranked_results = return_query_results(query1, 'field')\n",
    "\n",
    "        results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "        results = results[:num_results]\n",
    "\n",
    "        for id, _ in results:\n",
    "            title= title_search(id)\n",
    "            print(id+',', title)\n",
    "\n",
    "    else:\n",
    "        ranked_results = return_query_results(query1, 'simple')\n",
    "\n",
    "        results = sorted(ranked_results.items(), key = lambda item : item[1], reverse=True)\n",
    "        results = results[:num_results]\n",
    "\n",
    "        for id, _ in results:\n",
    "            title= title_search(id)\n",
    "            link = ('https://en.wikipedia.org/wiki/'+ title.title()).replace(' ', '_')\n",
    "            print(id+',', title+ '  ' +  link  )\n",
    "\n",
    "    e = time.time()\n",
    "    print('Finished in', e-s, 'seconds')\n",
    "    print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a5a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_english(query, num_results):\n",
    "    global stop_words\n",
    "    global num_pages\n",
    "    global lang \n",
    "    lang = 0\n",
    "    stop_words = (set(stopwords.words(\"english\")))\n",
    "\n",
    "    with open('english_wiki_index/num_pages.txt', 'r') as f:\n",
    "        num_pages = float(f.readline().strip())\n",
    "\n",
    "    temp = linecache.getline('english_wiki_index/id_title_map.txt', 0)\n",
    "    \n",
    "    take_input_from_user(query, num_results)\n",
    "\n",
    "def search_hindi(query, num_results):\n",
    "    global stop_words\n",
    "    global num_pages\n",
    "    global lang\n",
    "    global stem_words\n",
    "    lang = 1\n",
    "    query = query.strip()\n",
    "#     query = transliterate(query, sanscript.ITRANS, sanscript.DEVANAGARI)\n",
    "    with open('hindi_stopwords.txt', 'r', encoding='utf8') as f:\n",
    "        stop_words = [word.strip() for word in f]\n",
    "\n",
    "    with open('hindi_stem_words.txt', 'r',encoding='utf8') as f:\n",
    "        stem_words = [word.strip() for word in f]\n",
    "\n",
    "    with open('hindi_wiki_index/num_pages.txt', 'r',encoding='utf8') as f:\n",
    "        num_pages = float(f.readline().strip())\n",
    "\n",
    "    with open('hindi_wiki_index/num_tokens.txt', 'r',encoding='utf8') as f:\n",
    "        num_tokens = int(f.readline().strip())\n",
    "\n",
    "    tokens_info_pointer = 'hindi_wiki_index/tokens_info.txt'\n",
    "\n",
    "    temp = linecache.getline('hindi_wiki_index/id_title_map.txt', 0)\n",
    "#     take_input_from_user(query, num_results)\n",
    "    run_hindi_search(query,num_results)\n",
    "\n",
    "def run_query():\n",
    "    print('Loading search engine ')\n",
    "    print('Enter language keyword to search (e for English, h for Hindi), c to close')\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        \n",
    "        print('Querying: ')\n",
    "        num_results = 10\n",
    "        start = time.time()\n",
    "        query = input('Enter Query:- ')\n",
    "        language = query[0]\n",
    "        query = query[2:-1]\n",
    "        if language=='c':\n",
    "            return\n",
    "        if language=='h':\n",
    "            search_hindi(query, num_results)\n",
    "        else:\n",
    "            search_english(query, num_results)\n",
    "            take_input_from_user(query, num_results)\n",
    "        print('Query time:- ', time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1830a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading search engine \n",
      "Enter language keyword to search (e for English, h for Hindi), c to close\n",
      "Querying: \n",
      "Enter Query:- h:Bhaarat\n",
      "117554, विकिपीडिया:autowikibrowser/typos  https://hi.wikipedia.org/wiki/विकिपीडिया:autowikibrowser/typos\n",
      "169725, विकिपीडिया:चौपाल/पुरालेख 42  https://hi.wikipedia.org/wiki/विकिपीडिया:चौपाल/पुरालेख_42\n",
      "Query time:-  14.665740966796875\n",
      "Querying: \n",
      "Enter Query:- h:mahaajanpad\n",
      "5944, मगध महाजनपद  https://hi.wikipedia.org/wiki/मगध_महाजनपद\n",
      "8585, मैरी १ (स्कॉटलैंड की रानी)  https://hi.wikipedia.org/wiki/मैरी_१_(स्कॉटलैंड_की_रानी)\n",
      "82784, मैनहटन  https://hi.wikipedia.org/wiki/मैनहटन\n",
      "86604, मैसाचुसेट्स प्रौद्योगिकी संस्थान  https://hi.wikipedia.org/wiki/मैसाचुसेट्स_प्रौद्योगिकी_संस्थान\n",
      "6528, फ़्लोरिडा  https://hi.wikipedia.org/wiki/फ़्लोरिडा\n",
      "76639, फेडरल ब्यूरो ऑफ इन्वेस्टिगेशन  https://hi.wikipedia.org/wiki/फेडरल_ब्यूरो_ऑफ_इन्वेस्टिगेशन\n",
      "2445, भीमराव आम्बेडकर  https://hi.wikipedia.org/wiki/भीमराव_आम्बेडकर\n",
      "211828, निरंजन नागराजन  https://hi.wikipedia.org/wiki/निरंजन_नागराजन\n",
      "223050, मध्याह्न भोजन योजना  https://hi.wikipedia.org/wiki/मध्याह्न_भोजन_योजना\n",
      "148029, प्रमोद महाजन  https://hi.wikipedia.org/wiki/प्रमोद_महाजन\n",
      "Query time:-  10.020414352416992\n",
      "Querying: \n",
      "Enter Query:- e:windows\n",
      "397242, windows photo gallery  https://en.wikipedia.org/wiki/Windows_Photo_Gallery\n",
      "185739, windows media encoder  https://en.wikipedia.org/wiki/Windows_Media_Encoder\n",
      "56492, windows file manager  https://en.wikipedia.org/wiki/Windows_File_Manager\n",
      "47117, windows live  https://en.wikipedia.org/wiki/Windows_Live\n",
      "347011, windows workflow foundation  https://en.wikipedia.org/wiki/Windows_Workflow_Foundation\n",
      "134566, backup exec  https://en.wikipedia.org/wiki/Backup_Exec\n",
      "269284, windows driver kit  https://en.wikipedia.org/wiki/Windows_Driver_Kit\n",
      "200720, features new to windows xp  https://en.wikipedia.org/wiki/Features_New_To_Windows_Xp\n",
      "1359, windows mobile 5.0  https://en.wikipedia.org/wiki/Windows_Mobile_5.0\n",
      "309305, features new to windows vista  https://en.wikipedia.org/wiki/Features_New_To_Windows_Vista\n",
      "Finished in 0.006963491439819336 seconds\n",
      "\n",
      "397242, windows photo gallery  https://en.wikipedia.org/wiki/Windows_Photo_Gallery\n",
      "185739, windows media encoder  https://en.wikipedia.org/wiki/Windows_Media_Encoder\n",
      "56492, windows file manager  https://en.wikipedia.org/wiki/Windows_File_Manager\n",
      "47117, windows live  https://en.wikipedia.org/wiki/Windows_Live\n",
      "347011, windows workflow foundation  https://en.wikipedia.org/wiki/Windows_Workflow_Foundation\n",
      "134566, backup exec  https://en.wikipedia.org/wiki/Backup_Exec\n",
      "269284, windows driver kit  https://en.wikipedia.org/wiki/Windows_Driver_Kit\n",
      "200720, features new to windows xp  https://en.wikipedia.org/wiki/Features_New_To_Windows_Xp\n",
      "1359, windows mobile 5.0  https://en.wikipedia.org/wiki/Windows_Mobile_5.0\n",
      "309305, features new to windows vista  https://en.wikipedia.org/wiki/Features_New_To_Windows_Vista\n",
      "Finished in 0.005759477615356445 seconds\n",
      "\n",
      "Query time:-  6.589765548706055\n",
      "Querying: \n",
      "Enter Query:- c\n"
     ]
    }
   ],
   "source": [
    "run_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
